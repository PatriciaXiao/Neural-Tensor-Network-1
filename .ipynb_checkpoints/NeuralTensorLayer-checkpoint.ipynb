{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Loaded\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print ('Libraries Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](img1.png \"The Function modelled by the Neural Tensor Network\")\n",
    "\n",
    "This is the function modelled by the neural tensor layer where \n",
    "\n",
    "> * f is a standard nonlinearity applied element-wise, \n",
    "\n",
    "> * W<sup>[1:k]</sup><sub>R</sub>∈ R<sup>d×d×k</sup> is a tensor and the bilinear tensor product e<sup>T</sup><sub>1</sub>W<sup>[1:k]</sup><sub>R</sub>e2<sub>2</sub> results in a vector h ∈ R<sup>k</sup>, where each entry is computed by one slice i = 1, . . . , k of the tensor: hi = e<sup>T</sup><sub>1</sub>W<sup>[i]</sup><sub>R</sub>e2<sub>2</sub>. \n",
    "\n",
    "> * The other parameters for relation R are the standard form of a neural network: V<sub>R</sub>∈ R<sup>kx2d</sup>and U ∈ R<sup>k</sup>, b<sub>R</sub> ∈ R<sup>k</sup>.\n",
    "\n",
    "Four methods are required to be included in the class which will help to model the layer in keras :<br><br>\n",
    "* **__init()__** - This is used to initialise the layer.\n",
    "\n",
    "* **build(self, input_shape)** - Initialise the tensor variables and set the variables to be trained.\n",
    "\n",
    "* **call(self, x, mask=None)** - The forward pass operation is implemented here.\n",
    "\n",
    "* **get_output_shape_for(self, input_shape)** - Used to get the output shape before the network actually runs to help the building of the graph.\n",
    "\n",
    "![alt text](img2.png \"The Function modelled by the Neural Tensor Network\")\n",
    "\n",
    "**This is the block architecture of the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralTensorLayer(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, input_dim, activation= None):\n",
    "        super(NeuralTensorLayer, self).__init__()\n",
    "        self.output_dim = output_dim #The k in the formula\n",
    "        self.input_dim = input_dim   #The d in the formula\n",
    "        self.activation = activation #The f function in the formula\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        #The initialisation parameters\n",
    "        self.mean = 0.0 \n",
    "        self.stddev = 1.0\n",
    "        dtype = 'float32'\n",
    "        self.seed = 1\n",
    "        \n",
    "        #The output and the inut dimension\n",
    "        k = self.output_dim\n",
    "        d = self.input_dim\n",
    "        \n",
    "        #Initialise the variables to be trained. The variables are according to the\n",
    "        #function defined.\n",
    "        self.W = K.variable(K.random_normal((k,d,d), self.mean, self.stddev,\n",
    "                               dtype=dtype, seed=self.seed))\n",
    "        self.V = K.variable(K.random_normal((2*d,k), self.mean, self.stddev,\n",
    "                               dtype=dtype, seed=self.seed))\n",
    "        self.b = K.zeros((self.input_dim,))\n",
    "        \n",
    "        #Set the variables to be trained.\n",
    "        self.trainable_weights = [self.W, self.V, self.b]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        #Get Both the inputs\n",
    "        e1 = inputs[0]\n",
    "        e2 = inputs[1]\n",
    "        \n",
    "        #Get the batch size\n",
    "        batch_size = K.shape(e1)[0]\n",
    "        \n",
    "        #The output and the inut dimension\n",
    "        k = self.output_dim\n",
    "        d = self.input_dim\n",
    "\n",
    "        #The first term in the function which is the bilinear product is calculated here.\n",
    "        first_term_k = [K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1)]\n",
    "        for i in range(1, k):\n",
    "            temp = K.sum((e2 * K.dot(e1, self.W[i])) + self.b, axis=1)\n",
    "            first_term_k.append(temp)\n",
    "        first_term = K.reshape(K.concatenate(first_term_k, axis=0), (batch_size, k))\n",
    "\n",
    "        #The second term in the function is calculated here.\n",
    "        second_term = K.dot(K.concatenate([e1,e2]), self.V)\n",
    "        \n",
    "        #Sum of the two terms to get the final function\n",
    "        z =  first_term + second_term\n",
    "        \n",
    "        #The activation is selected here\n",
    "        if (self.activation == None):\n",
    "            return z\n",
    "        elif (self.activation == 'tanh'):\n",
    "            return K.tanh(z)\n",
    "        elif (self.activation == 'relu'):\n",
    "            return K.relu(z)\n",
    "        else :\n",
    "            print ('Activation not found')\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of Training Data: ', (1000, 300), (1000, 300), (1000, 1))\n",
      "('Shape of Validation Data', (100, 300), (100, 300), (100, 1))\n"
     ]
    }
   ],
   "source": [
    "# Dummy training data\n",
    "x_train1 = np.random.random((1000, 300))\n",
    "x_train2 = np.random.random((1000, 300))\n",
    "y_train = np.random.random((1000, 1))\n",
    "\n",
    "# Dummy validation data\n",
    "x_val1 = np.random.random((100, 300))\n",
    "x_val2 = np.random.random((100, 300))\n",
    "y_val = np.random.random((100, 1))\n",
    "\n",
    "print ('Shape of Training Data: ', x_train1.shape, x_train2.shape, y_train.shape)\n",
    "print ('Shape of Validation Data', x_val1.shape, x_val2.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super() takes at least 1 argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9837dd6c854d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvector2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m BilinearLayer = NeuralTensorLayer(output_dim=32, input_dim=300, \n\u001b[0;32m----> 5\u001b[0;31m                                   activation= 'relu')([vector1, vector2])\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#The g or the output of the modelled function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-65db7618cf88>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dim, input_dim, activation)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;31m#The k in the formula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dim\u001b[0m   \u001b[0;31m#The d in the formula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: super() takes at least 1 argument (0 given)"
     ]
    }
   ],
   "source": [
    "#Here Define the model\n",
    "vector1 = Input(shape=(300,), dtype='float32')\n",
    "vector2 = Input(shape=(300,), dtype='float32')\n",
    "BilinearLayer = NeuralTensorLayer(output_dim=32, input_dim=300, \n",
    "                                  activation= 'relu')([vector1, vector2])\n",
    "\n",
    "#The g or the output of the modelled function.\n",
    "g = Dense(output_dim=1)(BilinearLayer)\n",
    "model = Model(input=[vector1, vector2], output=[g])\n",
    "\n",
    "#Compile the model\n",
    "adam = optimizers.adam(.001)\n",
    "model.compile( loss='mean_squared_error', optimizer=adam)\n",
    "#The summary of the model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2105.7567 - val_loss: 338.3978\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 163.3842 - val_loss: 5.7870\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 5.9936 - val_loss: 0.9809\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.4667 - val_loss: 0.6592\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1.7644 - val_loss: 0.5543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa57cfc5dd8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train1, x_train2], y_train,\n",
    "          batch_size=64, epochs=5,\n",
    "          validation_data=([x_val1, x_val2], y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
